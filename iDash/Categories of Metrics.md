<ac:layout><ac:layout-section ac:type="single"><ac:layout-cell><p><br></p><p><br></p><h2>Desired Metrics + Categories</h2></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p><strong>NOTE:</strong> The metrics (below) have been identified to help a team measure &quot;important things&quot; - based on goals and their situation.</p><p>There are many potential pitfalls to metrics: over-focusing too closely on specific metrics, forgetting that they are just a proxy for performance. Metrics are especially damaging, when used for incentives - as teams might try to fudge numbers or &quot;game the system&quot;.</p><p>Metrics are best used to&nbsp;<strong>help the team get better</strong> - and metrics should be continually reviewed to ensure if they are still providing value - and if there might be any new metrics that may also provide value, to begin tracking too.</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="three_equal"><ac:layout-cell><p><u><strong><ac:inline-comment-marker ac:ref="54b11e14-f435-4edc-b60e-855bea308346">Application Metrics:</ac:inline-comment-marker></strong></u></p><ol><li><strong>Application Name</strong></li><li><strong>Application Availability / Uptime</strong> (time)</li><li><strong>Tech Debt</strong> (time)</li><li><strong>Test Coverage</strong>:<ol><li><strong>Unit Test (Code) Coverage</strong> %</li><li><strong>Integration Coverage</strong> %</li><li><strong>Functional Coverage</strong> %</li></ol></li><li><p class="p1">Test Status:</p><ol><li>Unit Tests (Number of Passing Tests / Total Number of Tests / Test Pass %)</li></ol></li><li><strong>Change Related Incidents</strong> (CRIs)</li><li><ac:inline-comment-marker ac:ref="d4da9e78-05ec-4570-b17a-6212550b83b7"><strong>Known Bugs/Issues (including Severity)</strong>&nbsp;(stretch goal)</ac:inline-comment-marker></li></ol><p class="listnumber">&nbsp;</p><p class="listnumber"><strong>Initial Goals:&nbsp;</strong></p><ul><li class="listnumber">Feasibility of POC</li><li class="listnumber">View and interpret dashboard data</li><li class="listnumber">Quality Metrics:&nbsp;<ul><li class="listnumber">Application Test Coverage, Test Status</li><li class="listnumber">Analyze application performance</li></ul></li></ul></ac:layout-cell><ac:layout-cell><p class="listnumber"><u><strong>Team/Agile Metrics:</strong></u></p><ol><li class="listnumber"><strong>Velocity</strong></li><li class="listnumber"><strong>Throughput</strong></li><li class="listnumber"><strong>Commitment Completion Percentage</strong></li><li class="listnumber"><strong>Volatility</strong>&nbsp;</li><li class="listnumber"><strong>Cycle Time</strong></li><li class="listnumber"><strong>MTTR / MTTD</strong></li><li class="listnumber"><strong>Number of Commits (per Build)</strong></li><li class="listnumber"><strong>Number of Code Reviews (per Commit)</strong></li></ol><p class="listnumber">&nbsp;</p><p class="listnumber"><strong>Additional Considerations:&nbsp;</strong></p><ul><li class="listnumber"><a href="https://confluence/display/ADD/Dashboard">TLPM</a> :&nbsp;<ac:link><ri:page ri:space-key="ADDArchive" ri:content-title="Dashboard"></ri:page></ac:link></li><li class="listnumber">Monitor team health</li><li class="listnumber">Monitor team capacity, and performance (work output)</li><li class="listnumber">Analyze team performance problems</li><li class="listnumber">View and interpret dashboard data</li><li class="listnumber">Team meta-data (owners, apps, servers, etc.)</li><li class="listnumber">Code Churn</li><li class="listnumber">Code Reviews effectiveness</li><li class="listnumber">Changes per build/release</li></ul></ac:layout-cell><ac:layout-cell><p><u><strong>Infrastructure</strong></u>/<u><strong>System Metrics (Post-Deployment/Server):</strong></u></p><ol class="listnumber"><li class="listnumber">CPU</li><li class="listnumber">Memory</li><li class="listnumber">Disk</li><li class="listnumber">Server errors (Splunk)</li><li class="listnumber">Response Times / Throughput (requests/min)</li></ol><p class="listnumber">&nbsp;</p><p class="listnumber"><strong>Additional Considerations:&nbsp;</strong></p><ul><li class="listnumber">Monitor system health / availability</li><li class="listnumber">Monitor application health and performance</li><li class="listnumber">Analyze application performance problems</li><li class="listnumber">View and interpret dashboard data</li></ul><p class="listnumber">&nbsp;</p><ul><li><strong>System Cycle Time:</strong>&nbsp;The time taken from a need being identified to fulfilling it. This is a measure of efficiency and speed of change management.&nbsp;</li><li><strong>Mean-Time to Recover (MTTR):</strong>&nbsp;The time taken from an availability problem (degraded performance or functionality) being identified to a resolution. This is a measure of efficiency and speed of problem resolution.&nbsp;</li><li><strong>Mean-Time Between Failures (MTBF):</strong>&nbsp;The time between critical availability failures. This is a measure of the stability of the system, and the quality of the change management process. (Note: over-optimizing for MTBF is a common cause of poor performance on other metrics)</li><li><strong>Availability:</strong>&nbsp;The percentage of time that a system is available, excluding the time a system is offline due to planned maintenance.</li><li><strong>True Availability:</strong>&nbsp;The percentage of time that a system is available, NOT excluding planned maintenance.</li></ul></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="three_equal"><ac:layout-cell><p><strong>Static Code Analysis (Pre-Commit):</strong></p><ul style="list-style-type: square;"><li>Quality Metrics:&nbsp;<ul style="list-style-type: square;"><li>Tech Debt</li><li>Code Complexity</li><li>Class Coupling</li><li class="listnumber">Duplicated Code</li><li class="listnumber">Code Churn</li></ul></li></ul><p><br></p></ac:layout-cell><ac:layout-cell><p><strong>User Experience:</strong></p><ul style="list-style-type: square;"><li>Quality Metrics:&nbsp;<ul style="list-style-type: square;"><li>Response times (at-the-glass / user experience)</li><li>Ease-of-Use</li><li>Uptime / Availability</li><li>Ease-of-maintenance/changes (Feature flagging)</li><li>User Experience Evaluations (Blue/Green Deployments)</li></ul></li></ul><p><br></p></ac:layout-cell><ac:layout-cell><p><strong>Additional Quality Metrics (for future consideration):</strong></p><ol><li><strong>Test Coverage</strong>:<br><ol><li><strong>Integration Coverage</strong>&nbsp;%</li><li><strong>Functional Coverage</strong>&nbsp;%</li></ol></li><li><p class="p1">Test Status:</p><ol><li>Integration Tests (Number of Passing Tests / Total Number of Tests / Test Pass %)</li><li>Functional Tests (Number of Passing Tests / Total Number of Tests / Test Pass %)</li></ol></li></ol></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><div class="wiki-content"><h1>Categorized Metrics + Data Points:</h1><table class="wrapped relative-table" style="width: 100.0%;"><colgroup><col style="width: 5.71895%;"><col style="width: 4.90196%;"><col style="width: 11.4379%;"><col style="width: 73.0095%;"><col style="width: 4.90196%;"></colgroup><thead><tr><th><div class="tablesorter-header-inner">Priority</div></th><th colspan="1"><div class="tablesorter-header-inner">Type</div></th><th><div class="tablesorter-header-inner">Name</div></th><th><div class="tablesorter-header-inner">Data Points</div></th><th><div class="tablesorter-header-inner">Notes</div></th></tr></thead><tbody><tr><td colspan="1">MVP</td><td colspan="1">APP</td><td colspan="1">Application Name</td><td colspan="1"><ul><li>Description of the application/service</li></ul></td><td colspan="1">&nbsp;</td></tr><tr><td>MVP</td><td colspan="1">APP</td><td>Application/Service Availability Status</td><td><div class="content-wrapper"><ul><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/tick-circle.png"></ri:url></ac:image></span>&nbsp;The app/service is up</li><li><p><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/cross-circle.png"></ri:url></ac:image></span>&nbsp;The&nbsp;app/service is currently down</p></li><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/exclamation.png"></ri:url></ac:image></span>&nbsp;The&nbsp;app/service is experiencing intermittent problems</li><li><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><ac:image ac:thumbnail="true" ac:width="16"><ri:attachment ri:filename="image2016-11-28 15:53:41.png"></ri:attachment></ac:image></span>&nbsp;The&nbsp;app/service is under-going maintenance</li></ul></div></td><td>&nbsp;</td></tr><tr><td colspan="1">MVP</td><td colspan="1">APP</td><td colspan="1"><p class="listnumber">Average Response Time (seconds)</p></td><td colspan="1"><div class="content-wrapper"><ul><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/tick-circle.png"></ri:url></ac:image></span>&nbsp;Excellent: Improved or similar to previous (day/week/month)</li><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/exclamation.png"></ri:url></ac:image></span>&nbsp;Fair: Increased by 10% since previous</li><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/cross-circle.png"></ri:url></ac:image></span>&nbsp;Poor: Increased by more than 10% since previous</li></ul></div></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">1</td><td colspan="1">APP</td><td colspan="1">Throughput (requests/min)</td><td colspan="1"><div class="content-wrapper"><ul><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/tick-circle.png"></ri:url></ac:image></span>&nbsp;Excellent: Improved or similar to previous (day/week/month)</li><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/exclamation.png"></ri:url></ac:image></span>&nbsp;Fair: Increased by 10% since previous</li><li><span class="confluence-embedded-file-wrapper"><ac:image ac:class="sym"><ri:url ri:value="http://stashboard.appspot.com/images/icons/fugue/cross-circle.png"></ri:url></ac:image></span>&nbsp;Poor: Increased by more than 10% since previous</li></ul></div></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">MVP</td><td colspan="1">APP</td><td colspan="1">Average Uptime (%)</td><td colspan="1"><ul><li>Excellent: 98%</li><li>Fair: Between 90% - 98%</li><li>Poor: Less than 90%</li></ul></td><td colspan="1">&nbsp;</td></tr><tr><td>1</td><td>APP</td><td>Minutes of System DownTime</td><td>Scheduled or unscheduled time system is down</td><td>&nbsp;</td></tr><tr><td>1</td><td>APP</td><td>Code Coverage</td><td>% of unit tests covering lines of code (versus uncovered lines of code / untested)<span> (SonarQube)</span></td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>Average Work Item Throughput</td><td>Throughput = time of inception (creation) to time of closing</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>Throughput per build</td><td>Throughput = time of inception (creation) to time of closing</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">APP</td><td>Production Bugs</td><td>Number of bugs NOT&nbsp;verified&nbsp;in production</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>Changes Per Build</td><td>Number of changes into post production</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>Patches Per Build</td><td>Number of patches/fixes into post production (subset of&nbsp;<u>Changes Per Build</u>)</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>New Bugs</td><td>Number of bug fixes per build</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>Code Freeze Adherence&nbsp;</td><td>% of Done vs. Committed at Code Freeze</td><td>&nbsp;</td></tr><tr><td>1</td><td colspan="1">TEAM</td><td>Drop Out Rate</td><td>Total Committed vs. Done at Sprint Completion</td><td>&nbsp;</td></tr><tr><td>?</td><td colspan="1">TEAM</td><td>Team Burndown</td><td>Tech team burn-down for current build</td><td>&nbsp;</td></tr><tr><td>?</td><td colspan="1">TEAM</td><td>Effort Targeting</td><td>Number of Work Items (Bugs, Stories, Analysis Items, Tech Debt) With Acceptance Criteria vs No Acceptance Criteria</td><td>&nbsp;</td></tr><tr><td><ac:inline-comment-marker ac:ref="f4e28849-52f2-4f98-8bd5-608ef74cc7b3">?</ac:inline-comment-marker></td><td colspan="1"><ac:inline-comment-marker ac:ref="f4e28849-52f2-4f98-8bd5-608ef74cc7b3">TEAM</ac:inline-comment-marker></td><td><ac:inline-comment-marker ac:ref="f4e28849-52f2-4f98-8bd5-608ef74cc7b3">Changes Per Team Member</ac:inline-comment-marker></td><td><ac:inline-comment-marker ac:ref="f4e28849-52f2-4f98-8bd5-608ef74cc7b3">Completed changes per team member</ac:inline-comment-marker></td><td>&nbsp;</td></tr><tr><td>?</td><td colspan="1">TEAM</td><td>Automation Growth</td><td>Number of new automated tests (unit tests, integration tests, GUI tests) for build</td><td>&nbsp;</td></tr><tr><td>?</td><td colspan="1">TEAM</td><td>Velocity Impact</td><td>Number of bugs in current build vs story points of previous build</td><td>&nbsp;</td></tr><tr><td colspan="1">?</td><td colspan="1">TEAM</td><td colspan="1">NFR Counts</td><td colspan="1">Number of NFR related Stories (per category: Documentation, Coverage, Security, Performance, Monitoring/Alerting)</td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">?</td><td colspan="1">APP</td><td colspan="1">Code Churn</td><td colspan="1"><p>Number of changes for a build:</p><ul><li>Added lines</li><li>Modified lines</li><li>Deleted lines</li></ul></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">!</td><td colspan="1">TEAM</td><td colspan="1">Successful change percentage</td><td colspan="1">The number of lines add/modified/changed in a change, that successfully were integrated into PROD</td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">!</td><td colspan="1">APP</td><td colspan="1">Number of 90Ks</td><td colspan="1">&nbsp;</td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">!</td><td colspan="1">APP</td><td colspan="1">Number of change related incidents</td><td colspan="1">&nbsp;</td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1">APP</td><td colspan="1">Automated Application Test Coverage</td><td colspan="1"><p>Consider looking into tools:&nbsp;<a class="external-link" rel="nofollow" href="https://smartbear.com/product/aqtime-pro/overview/">https://smartbear.com/product/aqtime-pro/overview/</a>&nbsp;&amp;&nbsp;<a href="http://www.semdesigns.com/Products/TestCoverage/" class="external-link" rel="nofollow">http://www.semdesigns.com/Products/TestCoverage/</a><br>We need to measure how much code is being touched by &quot;non-unit-test-TESTs&quot; - like when we run UI/Integration tests - how much of an application is being tested by &quot;functional tests&quot; that we run: manually or automated against a deployed environment</p><p>Whitepaper on tool evals:&nbsp;<a class="external-link" rel="nofollow" href="https://www.researchgate.net/publication/228922323_An_Evaluation_of_Test_Coverage_Tools_in_Software_Testing">https://www.researchgate.net/publication/228922323_An_Evaluation_of_Test_Coverage_Tools_in_Software_Testing</a></p><p><em>NOTE: Check out Gartner too</em></p></td><td colspan="1">&nbsp;</td></tr><tr><td>!</td><td>&nbsp;<span>APP</span></td><td><span>Average Response Time </span></td><td><span>The average time duration for a web page to respond to a user/client request (AppDynamics)</span></td><td>&nbsp;</td></tr><tr><td colspan="1">&nbsp;!</td><td colspan="1">&nbsp;<span>APP</span></td><td colspan="1"><p>Average Page Download Time</p></td><td colspan="1">The average time duration to download all assets for a web page (Google Analytics)</td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;!</td><td colspan="1">&nbsp;<span>APP</span></td><td colspan="1"><span>Average Page Load Time</span></td><td colspan="1"><span>The average time duration to load a web page</span><span> (Google Analytics)</span></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">!</td><td colspan="1">&nbsp;<span>APP</span></td><td colspan="1"><span>Average Time On Page</span></td><td colspan="1"><span><span>The average time duration that a user/client spends on a web page</span> </span><span>(Google Analytics)</span></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">!</td><td colspan="1">&nbsp;<span>APP</span></td><td colspan="1"><span>Page Load Time</span></td><td colspan="1"><span><span>The average time duration to load a web page </span></span><span>(Google Analytics)</span></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">!</td><td colspan="1">&nbsp;<span>APP</span></td><td colspan="1"><span>Unique Page Views</span></td><td colspan="1"><span><span>The number of unique users/clients visits to a specific web page</span> </span><span> (Google Analytics)</span></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;!</td><td colspan="1">&nbsp;APP</td><td colspan="1"><span>Violations</span></td><td colspan="1">The number of issues/bugs/code smells found in an application's source code (SonarQube)</td><td colspan="1">&nbsp;</td></tr><tr><td>&nbsp;!</td><td>&nbsp;APP</td><td><span>Complexity</span></td><td>Complexity is calculated based on the number of paths through the code. Whenever the control flow of a function splits, the complexity counter gets incremented by one. Each&nbsp;<strong>function&nbsp;</strong>has a minimum complexity of 1. This calculation varies slightly by language because keywords and functionalities do.<span> (SonarQube)</span></td><td>&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><a href="https://confluence/pages/viewpage.action?pageId=66292715">TEAM</a></td><td colspan="1">Velocity</td><td colspan="1"><h4><span style="color: rgb(51,51,153);"><u>Velocity</u></span></h4><ul><li style="list-style-type: none;background-image: none;"><ul><li><strong>Gump:</strong>&nbsp;How much stuff a team completes per sprint, measured in effort/size</li><li><strong>How We Use It:</strong>&nbsp;Used to help the team estimate how much work it can complete in a sprint based on how quickly similar work was previously completed</li><li><strong>How to Calculate:</strong>&nbsp;Calculated at the sprint level by summing the Effort of each product backlog item, including bugs and stories (and excluding analyses), that were marked as &lsquo;Done&rsquo; at the end of the sprint. Once a team has completed three sprints, the overall Velocity is calculated by averaging the last three sprints</li></ul></li></ul><p>(<a href="https://confluence/pages/viewpage.action?pageId=66292715">Team Level Performance Metrics (TLPM)</a></p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1">&nbsp;<a href="https://confluence/pages/viewpage.action?pageId=66292715">TEAM</a><span>&nbsp;</span></td><td colspan="1">Throughput</td><td colspan="1"><h4><span style="color: rgb(51,51,153);"><u>Throughput</u></span></h4><ul><li style="list-style-type: none;background-image: none;"><ul><li><strong>Gump:</strong>&nbsp;How much stuff a team completes per sprint, measured in count of items delivered</li><li><strong>How We Use It:</strong>&nbsp;Used to help the team estimate how much work it can complete in a sprint or specified time frame based on how quickly similar work was previously completed</li><li><strong>How to Calculate:</strong>&nbsp;Calculated at the sprint level by summing the number of product backlog items, including bugs and stories (and excluding analyses), that were marked as &lsquo;Done&rsquo; at the end of the sprint. Once a team has completed three sprints, the overall Throughput is calculated by averaging the last three sprints</li></ul></li></ul><p>(<a href="https://confluence/pages/viewpage.action?pageId=66292715">Team Level Performance Metrics (TLPM)</a></p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1">&nbsp;<a href="https://confluence/pages/viewpage.action?pageId=66292715">TEAM</a><span>&nbsp;</span></td><td colspan="1">Commitment Completion Percentage</td><td colspan="1"><h4><span style="color: rgb(51,51,153);"><u>Commitment Completion Percentage</u></span></h4><ul><li style="list-style-type: none;background-image: none;"><ul><li><strong>Gump:</strong>&nbsp;The percentage of stuff we completed vs. what we originally planned to complete in a sprint</li><li><strong>How We Use It:</strong>&nbsp;Used to help the team forecast the probability of the team meeting their commitment in a sprint based on how quickly similar work was previously completed</li><li><strong>How to Calculate:</strong>&nbsp;Calculated at the sprint level by taking the sum of planned product backlog items, including bugs and stories (and excluding analyses), that were marked as &lsquo;Done&rsquo; at the end of the iteration and dividing by the sum of planned product backlog items, including bugs and stories (and excluding analyses), that were committed to the current sprint</li></ul></li></ul><p>(<a href="https://confluence/pages/viewpage.action?pageId=66292715">Team Level Performance Metrics (TLPM)</a></p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><a href="https://confluence/pages/viewpage.action?pageId=66292715">TEAM</a><span>&nbsp;</span>&nbsp;</td><td colspan="1">Volatility</td><td colspan="1"><h4><span style="color: rgb(51,51,153);"><u>Volatility</u></span></h4><ul><li><strong>Gump:</strong>&nbsp;How much flux a team has in the amount of stuff we complete per sprint &ndash; the more flux, the less predictable the team&rsquo;s output</li><li><strong>How We Use It:</strong>&nbsp;Used to understand how predictable the team&rsquo;s velocity is and the degree to which the velocity has changed over time</li><li><strong>How to Calculate:</strong>&nbsp;Calculated at the sprint level by taking the difference between the current and previous sprint&rsquo;s velocity</li></ul><p><span>(</span><a href="https://confluence/pages/viewpage.action?pageId=66292715">Team Level Performance Metrics (TLPM)</a></p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><a href="https://confluence/pages/viewpage.action?pageId=66292715">TEAM</a><span>&nbsp;</span>&nbsp;</td><td colspan="1">Team Cycle Time</td><td colspan="1"><h4><span style="color: rgb(51,51,153);"><u>Cycle Time (Team)</u></span></h4><ul><li style="list-style-type: none;background-image: none;"><ul><li><strong>Gump:</strong>&nbsp;How many days it takes us to complete something, from the time we commit to it in a sprint until it&rsquo;s done</li><li><strong>How We Use It:</strong>&nbsp;Used to help the team forecast when the team will complete a product backlog item (a story or bug) based on how quickly similar work was previously completed</li><li><strong>How to Calculate:</strong>&nbsp;Calculated at the sprint level by averaging the days it took a team to complete a product backlog item, including bugs and stories (and excluding analyses), from the time it was marked &lsquo;Committed&rsquo; to the time it was marked as &lsquo;Done&rsquo; at the end of the sprint. Once a team has completed three sprints, the overall Cycle Time is calculated by averaging the last three sprints.</li></ul></li></ul><p><span>(</span><a href="https://confluence/pages/viewpage.action?pageId=66292715">Team Level Performance Metrics (TLPM)</a></p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1">SYSTEM</td><td colspan="1">System Cycle Time</td><td colspan="1"><p>The time taken from a need being identified to fulfilling it. This is a measure of efficiency and speed of change management.&nbsp;</p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><span>SYSTEM</span></td><td colspan="1">Mean-Time to Recover (MTTR)</td><td colspan="1"><p>The time taken from an availability problem (degraded performance or functionality) being identified to a resolution. This is a measure of efficiency and speed of problem resolution.&nbsp;</p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><span>SYSTEM</span></td><td colspan="1">Mean-Time Between Failures (MTBF)</td><td colspan="1"><p>The time between critical availability failures. This is a measure of the stability of the system, and the quality of the change management process. (Note: over-optimizing for MTBF is a common cause of poor performance on other metrics)</p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><span>SYSTEM</span></td><td colspan="1">Availability</td><td colspan="1"><p>The percentage of time that a system is available, excluding the time a system is offline due to planned maintenance.</p></td><td colspan="1">&nbsp;</td></tr><tr><td colspan="1">&nbsp;</td><td colspan="1"><span>SYSTEM</span></td><td colspan="1">True Availability</td><td colspan="1"><p>The percentage of time that a system is available, NOT excluding planned maintenance.</p></td><td colspan="1">&nbsp;</td></tr></tbody></table><p>&nbsp;</p><h2>Known Bugs/Issues : Severity</h2><table class="wrapped"><colgroup><col><col></colgroup><thead><tr><th><div class="tablesorter-header-inner">Severity</div></th><th><div class="tablesorter-header-inner">Description</div></th></tr></thead><tbody><tr><td><strong>Blocker</strong></td><td>Operational/security&nbsp;<strong><em>risk</em></strong>: This issue might make the whole application unstable in production. Ex: calling garbage collector, not closing a socket, etc.</td></tr><tr><td><strong>Critical</strong></td><td>Operational/security&nbsp;<strong><em>risk</em></strong>: This issue might lead to an unexpected behavior in production without impacting the integrity of the whole application. Ex: NullPointerException, badly caught exceptions, lack of unit tests, etc.</td></tr><tr><td><strong>Major</strong></td><td>This issue might have a substantial impact on&nbsp;<strong><em>productivity</em></strong>. Ex: too complex methods, package cycles, etc.</td></tr><tr><td><strong>Minor</strong></td><td>This issue might have a potential and minor impact on&nbsp;<strong><em>productivity</em></strong>. Ex: naming conventions, Finalizer does nothing but call superclass finalizer, etc.</td></tr><tr><td colspan="1"><strong>Info</strong></td><td colspan="1">Unknown or not yet well defined security risk or impact on productivity.&nbsp;</td></tr></tbody></table><p class="auto-cursor-target"><br></p></div></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><h2>Architectural Design Metrics:</h2><p><strong>Component coupling:</strong> Use interface characteristics like the numbers of input parameters, output parameters and global variables to create an algorithm for measuring component coupling. Once you have an algorithm in place, create automation that validates your source code's coupling in a continuous integration environment. The goal is to reduce coupling within your development objects.</p><p><strong>Interface versioning:</strong> Use a percentage to indicate the number of project components that have defined interfaces that can be and are versioned. Versioned interfaces allow teams to maintain different branches of functionality in a way that's seamless to users. The goal is to increase these numbers.</p><p><strong>Interface complexity:</strong> Use this metric to measure a component's interface by its complexity of maintenance. For instance, a REST API component may have less intensity than one with a brokered Web service or one using a Component Object Model (COM) interface. By creating components that have less complexity, teams can deploy small bits of functionality more often with less risk.</p><p><strong>Portability:</strong> This metric is a measure of how well-prepared an object is to change to new specifications, operating systems or platforms. This is a binary metric that corresponds to a list of your current platforms and that extends also to platforms you may choose to support in the future. For example, you may have a service object that lives in Java application server environment. You can support that object on any operating system that has an applicable application server. What happens when you want to move it to the cloud? Does it already have some cloudy characteristics?</p><p><strong>Separation of concerns:</strong> You can measure the separation of two concerns by intersecting the overall component coupling metric with the category of class or component. A category divides classes into distinctive groups with shared characteristics &mdash; such as libraries, hierarchies or external interactions. Much like with component coupling, use automation to determine whether source code has had its concern properly separated as part of a continuous integration effort. Finding architectural issues early means the team can fix problems before objects become actual dependencies.</p><p><strong>Published application programming interfaces (APIs):</strong> APIs give your users and developers the means to create functional applications without having to understand the intricacy and algorithms of back-end systems. Measure and collect data about the number of your systems that have published APIs and the percentage of functionality they expose in contrast to your graphical interfaces. By consistently increasing this number, you free your different teams to focus on their areas of expertise (for instance, presentation layers).</p><p><strong>Database objects:</strong> Complexity in applications increases with the number of database objects in an application cluster. The goal is to streamline and reduce the number of database objects in an application.</p><p><strong>Cyclic dependency:</strong> Objects may have a bidirectional dependency &mdash; for instance, Object A requires Object B, but Object B also requires Object A. This phenomena &mdash; known as cyclic dependency &mdash; prevents objects from being packaged separately and are difficult to maintain. Using an interval scale, measure the number of cyclic dependencies each object has, with the goal of lowering the number to zero.</p><p><strong>Number of parameters:</strong> Count the number of parameters that each method has. A large number of parameters makes for confusing development. If the number of parameters for a method is larger than four, consider a requirement for passing objects instead.&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><h2>Source Code Metrics:</h2><p><strong>Code reviews:</strong> This metric measures the percentage of source code that has been peer reviewed or developed in a pair development environment. The goal is to increase this number.</p><p><strong>Test reviews:</strong> This metric measures the percentage of test code (for both development unit tests and standard quality tests) that has been peer-reviewed or developed in a pair development environment. The goal is to increase this number.</p><p><strong>Source code standards:</strong> Use this metric to measure, as a percentage, the amount of source code that has been analyzed to be compliant with the organization's coding standards &mdash; such as those that address indent spacing, documentation requirements and naming conventions. The goal is to increase this number.</p><p><strong>Density of comments:</strong> This metric measures the number of lines of comments as a ratio to the total lines of code in the object. A standard goal is to aim for a 20% ratio. 10 Static code analysis: This metric measures the percentage of source code that has been analyzed for improper coding patterns by static analysis security and algorithm validation tools. The goal is to increase this number.</p><p><strong>Unused code:</strong> Using static analysis tools, track the instances of code that exists in an application but to which there are no pathways for the code to be called. Unused code bloats the pipeline and might introduce security risks. The goal is to lower this number to zero.</p><p><strong>Duplicated code:</strong> Using static analysis tools, track the instances of duplicated code so that it can be refactored into a separate method or object. You must maintain every instance of duplicated code when defects have been found. It's much more efficient to have changes made once that then cascade throughout the project. The goal is to lower this number to zero.</p><p><strong>Illegal code use:</strong> Track the number of times that source code is found that originated from a nonapproved copyrighted location (such as code copied from an open-source project) or that a component depends on a nonapproved library. The goal is to lower this number (and to remove the offending source code from the project).</p><p><strong>Source code cyclomatic complexity:</strong> This metric gauges the complexity of a program by measuring the number of linearly independent paths logic can take. Continuous inspection tools &mdash; such as SonarQube &mdash; measure such complexity. Complex code is difficult to maintain and test, in addition to increasing the likelihood of defects. The goal is to decrease the complexity of your applications.</p><p><strong>Code (unit test) coverage:</strong> Use this metric to state as a percentage the number of components that have full-validation unit test coverage. The goal is to increase this number.</p><p><strong>Continuous testing:</strong> This metric measures the percentage of source code that is part of a continuous integration system with full validation unit tests. The goal is to increase this number.</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><h2>Resource Metrics:</h2><p><strong>Number of commits:</strong> Developers should commit code to version control systems often to ensure that automated systems fully integrate source code in a continuous manner. Use this metric to track how often developers commit code to the version control system. The goal is to increase this number and ensure developers commit at least once per day.</p><p><strong>Ratio of issues to commits:</strong> As a general aggregation, consider dividing the number of issues by the number of commits and strive to lower this number.</p><p><strong>Team member load and capacity:</strong> By tracking individual member capacities and load, teams can gauge progress, avoid bottlenecks and manage resources more effectively. Such measurements also help teams estimate properly in the future. The goal with this metric is not to change its volume, but to collect the data for historical analysis. '</p><p><strong>Average work in progress:</strong> As teams move to visible workflows, they can see the number of work items in each &quot;swim lane&quot; associated with a process step. Collect the average work-in-progress metrics by determining the average number of items that exist in each step. Using the average, you can set work-in-progress limits or try to lower the number to a reasonable level.</p><p><strong>Time to detect:</strong> Teams use this metric to measure their ability to quickly detect problems that occur on production systems. The &quot;time&quot; variable measures the length of time from when the functionality was deployed to the date team members detected the problem. The goal is to decrease this number; teams should strive to keep this number under 24 hours.</p><p><strong>Time to resolve:</strong> Similar to &quot;time to detect,&quot; teams use this metric to calculate the time period from when a problem has been detected to the time that the problem was satisfactorily resolved. The goal is to decrease this number.</p><p><strong>Cost associated with defect:</strong> If a defect causes a delay or requires man hours to address, try to determine its associated financial burden. This metric will help teams better categorize defects in the future as well as allowing accountants to determine a project's return on investment.</p><p><strong>Test cycle times:</strong> Use this metric to measure the amount of time each test cycle takes for specific products, components and classes. The goal is to decrease this number.</p><p><strong>Defect age:</strong> The defect age metric provides an interval range of values depending on the phase in which the defect was introduced or the environment in which it was found. For instance, you might use the following scale to determine the &quot;age&quot; of a defect: Requirement: 0; Design: 1; Development: 2; Test: 3; Preproduction: 4; Production: 5. The scale allows you to determine escape rates.</p><p><strong>Defect escape rate:</strong> Use this metric to measure the number of defects found in different environments (such as development, test, preproduction and production). The goal should be to capture as many defects as possible in the earlier environments and have few defects escape to production systems. This measurement is much easier to ascertain if you use an agreed-upon defect age table.</p><p><strong> Smoke test escape:</strong> Use this metric to determine how many blocking bugs escape your basic smoke tests. The goal is to decrease this number.</p><p><strong>Spoilage:</strong> This metric allows teams to aggregate escape rates into a single number that can be monitored as a trend. To determine spoilage, multiply the defect age by the number of defects found in that environment and then divide that total by the total number of defects found in all phases. You can add the spoilage numbers for each phase together to see an aggregate total that indicates your total spoilage level.</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><h2>Environment Metrics:</h2><p><strong>Issues related to configuration:</strong> This metric relates to the number of issues that occur due to mistakes in configuring the application's environment. Applications are made up of more than just source code; process and environment configurations play major roles in providing functionality to users. You can better understand the resources and problems related to your environment configurations by tracking associated defects.</p><p><strong>Issues related to data center:</strong> This metric measures the number of issues that occur due to data center downtimes, bandwidth limits or cloud providers. As with the metrics associated with issues related to configuration, this metric helps you pinpoint where you might have data center problems that must be addressed. The goal is to lower this number.</p><p><strong>Issues related to traffic:</strong> This metric measures the number of issues that occur due to an increase of network traffic, application use or general performance. How your customers perceive your product is directly related to how well it performs. By breaking out performance metrics, you can better understand where you might need to increase expertise or expensive tuning resources. The goal is to lower this number.</p><p><strong>Issues related to environment parity:</strong> This metric measures the number of issues that occur due to differences in test environments. Nothing spins your team's wheels faster than a subtle difference in environment that has no correlation to the quality or performance of the product. The goal is to lower this number by identifying streamlined and automated ways to maintain parity between environments.</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><h2>Automation Metrics:</h2><p><strong>Automated build functionality:</strong> This metric counts the percentage of project components that can be built automatically in a repeatable fashion. Automation increases efficiency, reduces the risk of manual errors and is self-documenting. Your goal should be to continuously increase this percentage.</p><p><strong>Broken builds:</strong> This measurement counts how often build automation fails in a given time period. A broken build is one that does not compile, fails unit tests or is generally unusable by the test team. Broken builds are indicative of sloppy and untrained development teams. The goal is to decrease this number.</p><p><strong>Automated test functionality:</strong> This metric counts the percentage of project components whose bulk of functionality can be tested automatically in a repeatable fashion. The goal is to increase this percentage.</p><p><strong>Configuration and infrastructure automation:</strong> Use this metric to count the percentage of components whose configuration and test environments are created through automation. The goal should be to increase this number.</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><h2>Quality Metrics:</h2><p><strong>Number of defects:</strong> Use this metric to track the overall number and severity of defects in a project. An alternative form of this metric is to measure defect density. Defect density is the number of defects found in a specific time period divided by the size of the module (for example, lines of code). The goal is to have this number be relatively higher at the start of a project and much lower as the project reaches deployment.</p><p><strong>Issues related to documentation:</strong> Use this number as a way to track issues that resulted from the associated documentation being incomplete or incorrect. The goal of this metric is to decrease the number</p><p><strong>Integration and system test coverage:</strong> Integration test coverage is a measurement that describes the degree to which integrated components have been validated together by particular test suites. The definition of coverage is subjective, and teams must create specifications that determine what is and what is not coverage. For instance, a team may decide that full regression test coverage fully validates inputs and outputs for every component. In another example, teams might determine that unit tests must cover every component function, statement and condition. The goal is to specify what coverage means to your organization and then increase the percentage of components that are covered.</p><p><strong>Reusable tests:</strong> This metric measures the number of tests that support different kinds of testing. For instance, by using dynamic injection, teams can use unit tests for both isolation and integration testing. The goal is to increase this number.</p><p><strong>Test efficacy:</strong> Use this metric to determine the type of tests that found issues. For instance, how many issues were found due to unit testing versus regression testing? The goal is to try to find more issues during the development cycle than during late-phase testing.</p><p><strong>Dynamic and performance testing:</strong> Dynamic static analysis tools test applications at runtime, searching for security and performance problems. Load test applications simulate a virtual load on your application to determine performance issues. Use these tools' outputs as metrics to find issues. The goal of this metric is to increase the number of issues found early in the project life cycle and decrease that number as the project nears deployment.</p><p><strong>Defects based on tester error:</strong> At times, testers will report issues that turn out to work as designed. In these cases, you must understand whether the issue emerged due to a tester's lack of experience or judgment. In those cases, use the opportunity to mentor testers on more accurate methods of testing the product. In the case where the tester did not understand the functionality, consider whether the user experience needs to be corrected or whether the tester needs education regarding the product domain. The goal is to lower this number over time</p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type="single"><ac:layout-cell><p>&nbsp;</p></ac:layout-cell></ac:layout-section></ac:layout>